{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# from Controller import Controller, Controller_sequence, Controller_pure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "from multiprocessing import Pool, cpu_count, Process\n",
    "import multiprocessing\n",
    "# from utils import mod_column, evaluate, init_name_and_log, save_result\n",
    "from collections import ChainMap\n",
    "from subprocess import Popen, PIPE\n",
    "from time import time, sleep\n",
    "import os\n",
    "# from Java_service import start_service_pool, stop_service_pool, find_free_port\n",
    "# import rpyc\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(actions):\n",
    "\tglobal path, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package, method, origin_result\n",
    "\tX = pd.read_csv(path)\n",
    "\tnum_feature = X.shape[1] - 1\n",
    "\taction_per_feature = int(len(actions) / num_feature)\n",
    "\tcopies, copies_run, rewards = {}, [], []\n",
    "\n",
    "\tfor feature_count in range(num_feature):\n",
    "\t\tfeature_name = X.columns[feature_count]\n",
    "\t\tfeature_actions = actions[feature_count*action_per_feature: \\\n",
    "\t\t\t(feature_count+1)*action_per_feature]\n",
    "\t\tcopies[feature_count] = []\n",
    "\t\tif feature_actions[0] == 0:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tcopy = np.array(X[feature_name].values)\t\t\n",
    "\n",
    "\t\tfor action in feature_actions:\n",
    "\t\t\tif action == 0:\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif action > 0 and action <= num_op_unary:\n",
    "\t\t\t\taction_unary = action - 1\n",
    "\t\t\t\tif action_unary == 0:\n",
    "\t\t\t\t\tcopy = np.squeeze(np.sqrt(abs(copy)))\n",
    "\t\t\t\telif action_unary == 1:\n",
    "\t\t\t\t\tscaler = MinMaxScaler()\n",
    "\t\t\t\t\tcopy = np.squeeze(scaler.fit_transform(np.reshape(copy,[-1,1])))\n",
    "\t\t\t\telif action_unary == 2:\n",
    "\t\t\t\t\twhile (np.any(copy == 0)):\n",
    "\t\t\t\t\t\tcopy = copy + 1e-5\n",
    "\t\t\t\t\tcopy = np.squeeze(np.log(abs(np.array(copy))))\n",
    "\t\t\t\telif action_unary == 3:\n",
    "\t\t\t\t\twhile (np.any(copy == 0)):\n",
    "\t\t\t\t\t\tcopy = copy + 1e-5\n",
    "\t\t\t\t\tcopy = np.squeeze(1 / (np.array(copy))) \n",
    "\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\taction_binary = (action-num_op_unary-1) // (num_feature-1)\n",
    "\t\t\t\trank = np.mod(action-num_op_unary-1, num_feature-1)\n",
    "\n",
    "\t\t\t\tif rank >= feature_count:\n",
    "\t\t\t\t\trank += 1\n",
    "\t\t\t\ttarget_feature_name = X.columns[rank]\n",
    "\n",
    "\t\t\t\ttarget = np.array(X[target_feature_name].values) \n",
    "\n",
    "\t\t\t\tif action_binary == 0:\n",
    "\t\t\t\t\tcopy = np.squeeze(copy + target)\n",
    "\t\t\t\telif action_binary == 1:\n",
    "\t\t\t\t\tcopy = np.squeeze(copy - target)\n",
    "\t\t\t\telif action_binary == 2:\n",
    "\t\t\t\t\tcopy = np.squeeze(copy * target)\n",
    "\t\t\t\telif action_binary == 3:\n",
    "\t\t\t\t\twhile (np.any(target == 0)):\n",
    "\t\t\t\t\t\ttarget = target + 1e-5\n",
    "\t\t\t\t\tcopy = np.squeeze(copy / target) \n",
    "\t\t\t\telif action_binary == 4:\n",
    "\t\t\t\t\tcopy = np.squeeze(mod_column(copy, X[target_feature_name].values))\n",
    "\n",
    "\t\t\tcopies[feature_count].append(copy)\n",
    "\t\tcopies_run.append(copy)\n",
    "\n",
    "\tif method == 'train':\n",
    "\t\tformer_result = origin_result\n",
    "\t\tformer_copys = [None]\n",
    "\t\tfor key in sorted(copies.keys()):\n",
    "\t\t\treward, former_result, return_copy = get_reward_per_feature( \n",
    "\t\t\t\tcopies[key], action_per_feature, former_result, former_copys)\n",
    "\t\t\tformer_copys.append(return_copy)\n",
    "\t\t\trewards += reward\n",
    "\t\treturn rewards\n",
    "\t\n",
    "\telif method == 'test':\n",
    "\t\tfor i in range(len(copies_run)):\n",
    "\t\t\tX.insert(0, 'new%d'%i, copies_run[i])\n",
    "\t\tif package == 'weka':\n",
    "\t\t\tresult = get_weka_result(X)\n",
    "\t\telif package == 'sklearn':\n",
    "\t\t\ty = X[X.columns[-1]]\n",
    "\t\t\tdel X[X.columns[-1]]\n",
    "\t\t\tresult = evaluate1(X, y, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package)\n",
    "\t\treturn result\n",
    "\t\t\n",
    "\n",
    "def get_reward_per_feature(copies, count, former_result, former_copys=[None]):\n",
    "\tglobal path, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package\n",
    "\tX = pd.read_csv(path)\n",
    "\tif package == 'sklearn':\n",
    "\t\ty = X[X.columns[-1]]\n",
    "\t\tdel X[X.columns[-1]]\n",
    "\n",
    "\treward = []\n",
    "\tprevious_result = former_result\n",
    "\tfor i,former_copy in enumerate(former_copys):\n",
    "\t\tif not former_copy is None:\n",
    "\t\t\tX.insert(0, 'former%d'%i, former_copy)\n",
    "\n",
    "\tfor copy in copies:\n",
    "\t\tX.insert(0, 'new', copy)\n",
    "\t\tif package == 'weka':\n",
    "\t\t\tcurrent_result = get_weka_result(X)\n",
    "\t\telif package == 'sklearn':\n",
    "\t\t\tcurrent_result = evaluate1(X, y, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package)\n",
    "\n",
    "\t\treward.append(current_result - previous_result)\n",
    "\t\tprevious_result = current_result\n",
    "\t\tdel X['new']\n",
    "\n",
    "\treward_till_now = len(reward)\n",
    "\tfor _ in range(count - reward_till_now):\n",
    "\t\treward.append(0)\n",
    "\tif len(copies) == 0:\n",
    "\t\treturn_copy = None\n",
    "\telse:\n",
    "\t\treturn_copy = copies[-1]\n",
    "\n",
    "\treturn reward, previous_result, return_copy\n",
    "\n",
    "def random_run(nnum_random_sample, nmodel, l=None, p=None):\n",
    "\tglobal num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package, num_process\n",
    "\tsamples = []\n",
    "\tfor i in range(nnum_random_sample):\n",
    "\t\tsample = []\n",
    "\t\tfor _ in range(nmodel.num_action):\n",
    "\t\t\tsample.append(np.random.randint(nmodel.num_op))\n",
    "\t\tsamples.append(sample)\n",
    "\n",
    "\tif multiprocessing:\t\n",
    "\t\t# if package == 'weka':\n",
    "\t\t# \tpool = Pool(num_process, initializer=init, initargs=(l, p))\n",
    "\t\t# elif package == 'sklearn':\n",
    "\t\tif package == 'sklearn':\n",
    "\t\t\tpool = Pool(num_process)    \n",
    "\t\tres = list(pool.map(get_reward, samples))\n",
    "\t\tpool.close()\n",
    "\t\tpool.join()\n",
    "\telse:\n",
    "\t\tres = []\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tres.append(get_reward(sample))\n",
    "\n",
    "\trandom_result = max(res)\n",
    "\trandom_sample = samples[res.index(random_result)]\n",
    "\n",
    "\treturn random_result, random_sample\n",
    "\n",
    "\n",
    "def train(nmodel, l=None, p=None):\n",
    "\tglobal path, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package, infos, method, origin_result, num_process\n",
    "\n",
    "\tX = pd.read_csv(path)\n",
    "\tprint(X)\n",
    "\t# if package == 'weka':\n",
    "\t# \torigin_result = get_weka_result(X)\n",
    "\t# elif package == 'sklearn':\n",
    "\tif package == 'sklearn':\n",
    "\t\ty = X[X.columns[-1]]\n",
    "\t\tdel X[X.columns[-1]]\n",
    "\t\tprint(X.shape)\n",
    "\n",
    "\t\torigin_result = evaluate1(X, y, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package)\t\n",
    "\tbest_result = origin_result\n",
    "\tprint(origin_result)\n",
    "\n",
    "\tconfig = tf.ConfigProto()\n",
    "\tconfig.gpu_options.allow_growth = True\n",
    "\twith tf.Session(config=config) as sess:\n",
    "\t\tinit_op = tf.group(tf.global_variables_initializer(), \n",
    "\t\t\ttf.local_variables_initializer())\n",
    "\t\tsess.run(init_op)\n",
    "\t\t\n",
    "\t\tnmodel_result = -10000.0 \n",
    "\t\ttrain_set, values = [], []\n",
    "\t\tfor epoch_count in range(epochs):\n",
    "\t\t\tconcat_action = []\n",
    "\t\t\tprobs_action = sess.run(tf.nn.softmax(nmodel.concat_output))\n",
    "\n",
    "\t\t\tfor batch_count in range(num_batch):\n",
    "\t\t\t\tbatch_action = []\n",
    "\t\t\t\tfor i in range(probs_action.shape[0]):\n",
    "\t\t\t\t\tsample_action = np.random.choice(len(probs_action[i]), p=probs_action[i])\n",
    "\t\t\t\t\tbatch_action.append(sample_action)\n",
    "\t\t\t\tconcat_action.append(batch_action)\n",
    "\t\t\t\t\t\n",
    "\t\t\tmethod = 'train'\n",
    "\t\t\tif multiprocessing:\n",
    "\t\t\t\t# if package == 'weka':\n",
    "\t\t\t\t# \tpool = Pool(num_process, initializer=init, initargs=(l, p))  \n",
    "\t\t\t\t# elif package == 'sklearn':\n",
    "\t\t\t\tif package == 'sklearn':\n",
    "\t\t\t\t\tpool = Pool(num_process)        \n",
    "\t\t\t\trewards = np.array(pool.map(get_reward, concat_action))\n",
    "\t\t\t\tpool.close()\n",
    "\t\t\t\tpool.join()\n",
    "\t\t\telse:\n",
    "\t\t\t\trewards = []\n",
    "\t\t\t\tfor action in concat_action:\n",
    "\t\t\t\t\trewards.append(get_reward(action))\n",
    "\t\t\t\trewards = np.array(rewards)\n",
    "\n",
    "\t\t\tmethod = 'test'\n",
    "\t\t\tif multiprocessing:\n",
    "\t\t\t\t# if package == 'weka':\n",
    "\t\t\t\t# \tpool = Pool(num_process, initializer=init, initargs=(l, p))  \n",
    "\t\t\t\t# elif package == 'sklearn':\n",
    "\t\t\t\tif package == 'sklearn':\n",
    "\t\t\t\t\tpool = Pool(num_process)      \n",
    "\t\t\t\tresults = pool.map(get_reword, concat_action)\n",
    "\t\t\t\tpool.close()\n",
    "\t\t\t\tpool.join()\n",
    "\t\t\telse:\n",
    "\t\t\t\tresults = []\n",
    "\t\t\t\tfor action in concat_action:\n",
    "\t\t\t\t\tresults.append(get_reword(action))\n",
    "\t\t\tnmodel_result = max(nmodel_result, max(results))\n",
    "\n",
    "\n",
    "\t\t\tif RL_model == 'AC':\n",
    "\t\t\t\ttarget_set = []\n",
    "\t\t\t\tfor batch_count in range(num_batch):\n",
    "\t\t\t\t\taction = concat_action[batch_count]\n",
    "\t\t\t\t\tfor i in range(nmodel.num_action):\n",
    "\t\t\t\t\t\ttrain_tmp = list(np.zeros(nmodel.num_action, dtype=int))\n",
    "\t\t\t\t\t\ttarget_tmp = list(np.zeros(nmodel.num_action, dtype=int))\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\ttrain_tmp[0:i] = list(action[0:i])\n",
    "\t\t\t\t\t\ttarget_tmp[0:i+1] = list(action[0:i+1])\n",
    "\n",
    "\t\t\t\t\t\ttrain_set.append(train_tmp)\n",
    "\t\t\t\t\t\ttarget_set.append(target_tmp)\n",
    "\n",
    "\t\t\t\tstate = np.reshape(train_set, [-1,nmodel.num_action])\n",
    "\t\t\t\tnext_state = np.reshape(target_set, [-1,nmodel.num_action])\n",
    "\n",
    "\t\t\t\tvalue = nmodel.predict_value(next_state) * alpha + rewards.flatten()\n",
    "\t\t\t\tvalues += list(value)\n",
    "\t\t\t\tnmodel.update_value(state, values)\n",
    "\n",
    "\t\t\t\trewards_predict = nmodel.predict_value(next_state) * alpha - \\\n",
    "\t\t\t\t\tnmodel.predict_value(state[-np.shape(next_state)[0]:]) + rewards.flatten()\n",
    "\t\t\t\trewards = np.reshape(rewards_predict, [num_batch,-1])\n",
    "\n",
    "\t\t\t\n",
    "\t\t\telif RL_model == 'PG':\n",
    "\t\t\t\tfor i in range(nmodel.num_action):\n",
    "\t\t\t\t\tbase = rewards[:,i:]\n",
    "\t\t\t\t\trewards_order = np.zeros_like(rewards[:,i])\n",
    "\t\t\t\t\tfor j in range(base.shape[1]):\n",
    "\t\t\t\t\t\torder = j + 1\n",
    "\t\t\t\t\t\tbase_order = base[:,0:order]\n",
    "\t\t\t\t\t\talphas = []\n",
    "\t\t\t\t\t\tfor o in range(order):\n",
    "\t\t\t\t\t\t\talphas.append(pow(alpha, o))\n",
    "\t\t\t\t\t\tbase_order = np.sum(base_order*alphas, axis=1)\n",
    "\t\t\t\t\t\tbase_order = base_order * np.power(lambd, j) \n",
    "\t\t\t\t\t\trewards_order = rewards_order.astype(float) \n",
    "\t\t\t\t\t\trewards_order += base_order.astype(float) \n",
    "\t\t\t\t\trewards[:,i] = (1-lambd) * rewards_order\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\tfeed_dict = {nmodel.concat_action: np.reshape(concat_action, [num_batch,-1]), \\\n",
    "\t\t\t\tnmodel.rewards: np.reshape(rewards,[num_batch,-1])}\n",
    "\t\t\tloss_epoch = nmodel.update_policy(feed_dict, sess)\n",
    "\n",
    "\n",
    "\t\t\tmethod = 'test'\n",
    "\t\t\tprobs_action = sess.run(tf.nn.softmax(nmodel.concat_output))\n",
    "\t\t\tbest_action = probs_action.argmax(axis=1)\n",
    "\t\t\tnmodel_result = max(nmodel_result, get_reword(best_action))\n",
    "\n",
    "\t\t\trandom_result, random_sample = random_run(num_random_sample, nmodel, l, p)\n",
    "\n",
    "\t\t\tbest_result = max(best_result, nmodel_result)\n",
    "\n",
    "\t\t\tprint('Epoch %d: loss = %.4f, origin_result = %.4f, lr = %.3f, \\n model_result = %.4f, best_action = %s, \\n best_result = %.4f, random_result = %.4f, random_sample = %s' \n",
    "\t\t\t\t% (epoch_count+1, loss_epoch, origin_result, lr, nmodel_result, str(best_action), best_result, random_result, str(random_sample)))\n",
    "\t\t\tlogging.info('Epoch %d: loss = %.4f, origin_result = %.4f, lr = %.3f, \\n model_result = %.4f, best_action = %s, \\n best_result = %.4f, random_result = %.4f, random_sample = %s' \n",
    "\t\t\t\t% (epoch_count+1, loss_epoch, origin_result, lr, nmodel_result, str(best_action), best_result, random_result, str(random_sample)))\n",
    "\n",
    "\t\t\tinfo = [epoch_count, loss_epoch, origin_result, nmodel_result, random_result]\n",
    "\t\t\tinfos.append(info)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pnum_op_unary=4, pnum_op_binary=5, pmax_order=5, pnum_batch=32, poptimizer='adam', \n",
    "         plr=0.01, pepochs=100, pevaluate='r2', ptask='regression', pdataset='airfoil', \n",
    "         pmodel='RF', palpha=0.99, plr_value=1e-3, pRL_model='PG', preg=1e-5, \n",
    "         pcontroller='rnn', pnum_random_sample=5, plambd=0.4, pmultiprocessing=True, ppackage='sklearn'):\n",
    "\tglobal num_process, infos\n",
    "\tglobal path, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package\n",
    "\tnum_op_unary = pnum_op_unary\n",
    "\tnum_op_binary = pnum_op_binary\n",
    "\tmax_order = pmax_order\n",
    "\tnum_batch = pnum_batch\n",
    "\toptimizer = poptimizer\n",
    "\tlr = plr\n",
    "\tepochs = pepochs\n",
    "\tevaluate = pevaluate\n",
    "\ttask = ptask\n",
    "\tdataset = pdataset\n",
    "\tmodel = pmodel\n",
    "\talpha = palpha\n",
    "\tlr_value = plr_value\n",
    "\tRL_model = pRL_model\n",
    "\treg = preg\n",
    "\tcontroller = pcontroller\n",
    "\tnum_random_sample = pnum_random_sample\n",
    "\tlambd = plambd\n",
    "\tmultiprocessing = pmultiprocessing\n",
    "\tpackage = ppackage\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\t# args = parse_args()\n",
    "\torigin_result, method, name = None, None, None\n",
    "\tnum_process, infos = 64, []\n",
    "\t# num_weka_process = num_process\n",
    "\tname = init_name_and_log(num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package)\n",
    "\tprint(name)\n",
    "\n",
    "\n",
    "\tpath = '/content/' + dataset + '.csv'\n",
    "\tnum_feature = pd.read_csv(path).shape[1] - 1\n",
    "\tif controller == 'rnn':\n",
    "\t\tcontroller = Controller(num_op_unary, num_op_binary, max_order, num_batch, optimizer, \n",
    "         lr, epochs, evaluate, task, dataset, \n",
    "         model, alpha, lr_value, RL_model, reg, \n",
    "         controller, num_random_sample, lambd, multiprocessing, package, num_feature)\n",
    "\t# elif controller == 'pure':\n",
    "\t# \tcontroller = Controller_pure(args, num_feature)\n",
    "\tcontroller.build_graph()\n",
    "\n",
    "\t# if package == 'weka':\n",
    "\t# \ttrain(controller, lock, ports)\n",
    "\t# elif package == 'sklearn':\n",
    "\t# \ttrain(controller)\n",
    "\ttrain(controller)\n",
    "\n",
    "\tsave_result(infos, name)\n",
    "\t\n",
    "\t# if args.package == 'weka':\n",
    "\t# \tstop_service_pool(pids)\n",
    "\t# \tfor port in conns.keys():\n",
    "\t# \t\tconns[port].close()\n",
    "\tprint(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global path, num_op_unary, num_op_binary, max_order, num_batch, optimizer, lr, epochs, evaluate, task, dataset, model, alpha, lr_value, RL_model, reg, controller, num_random_sample, lambd, multiprocessing, package, method, origin_result, num_process, infos\n",
    "\n",
    "# need to implement 1-rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'init_name_and_log' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-73c7b2c93192>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# BMI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BMI'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RF'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PG'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rnn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sklearn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e5446a597c76>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(pnum_op_unary, pnum_op_binary, pmax_order, pnum_batch, poptimizer, plr, pepochs, pevaluate, ptask, pdataset, pmodel, palpha, plr_value, pRL_model, preg, pcontroller, pnum_random_sample, plambd, pmultiprocessing, ppackage)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mnum_process\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# num_weka_process = num_process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_name_and_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_op_unary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_op_binary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_order\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRL_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_random_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'init_name_and_log' is not defined"
     ]
    }
   ],
   "source": [
    "# BMI\n",
    "main(5, 4, 5, 32, 'adam', 0.01, 10, 'r2', 'regression', 'BMI', 'RF', 0.99, 1e-3, 'PG', 1e-5, 'rnn', 5, 0.4, True, 'sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airfoil\n",
    "\n",
    "main(5, 4, 5, 32, 'adam', 0.01, 10, 'r2', 'regression', 'airfoil', 'RF', 0.99, 1e-3, 'PG', 1e-5, 'rnn', 5, 0.4, True, 'sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}